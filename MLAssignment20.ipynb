{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "que1- What is the underlying concept of Support Vector Machines ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d3b8fe",
   "metadata": {},
   "source": [
    "ans1- The underlying concept of Support Vector Machines (SVM) is to find an optimal hyperplane that can effectively separate data points belonging to different classes in a high-dimensional feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "que2- What is the concept of a support vector ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d554df7",
   "metadata": {},
   "source": [
    "ans2- The concept of support vectors is based on the idea that these specific data points have the most influence on the separation of classes and the generalization ability of the model. They are the critical elements that define the margin, which is the distance between the decision boundary and the nearest data points of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "que3- When using SVMs, why is it necessary to scale the inputs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5b8cf",
   "metadata": {},
   "source": [
    "ans3- When using Support Vector Machines (SVMs), it is necessary to scale the inputs (features) to ensure accurate and reliable results. There are a few reasons why scaling is important in SVMs:\n",
    "\n",
    "- **Influence of Feature Magnitudes:** SVMs aim to find an optimal decision boundary that maximizes the margin between classes. If the features have significantly different magnitudes or scales, the SVM algorithm may assign excessive importance to features with larger values.\n",
    "\n",
    "- **Numerical Stability:** The SVM algorithm involves solving optimization problems and performing calculations that can be sensitive to the numerical values of the features. Scaling the inputs helps improve the numerical stability of the optimization process. It reduces the chances of numerical errors, such as overflow or underflow, and ensures more reliable and consistent results.\n",
    "\n",
    "- **Convergence Speed:** Scaling the inputs can also help improve the convergence speed of the SVM algorithm. With scaled features, the optimization process can converge faster as the algorithm can take more balanced steps towards the optimal solution. This is particularly important when dealing with large datasets or complex feature spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "que4- When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276bb236",
   "metadata": {},
   "source": [
    "ans4- SVM classifier can provide a confidence score, but it does not directly output a percentage chance. The confidence score is typically based on the distance of a data point from the decision boundary. Larger distances indicate higher confidence in the prediction. However, to obtain probability estimates, additional techniques like Platt scaling or sigmoid calibration can be used to transform the SVM outputs into interpretable probabilities. These methods allow the SVM classifier to provide a percentage chance or probability estimate for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "que5- Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad9ca1",
   "metadata": {},
   "source": [
    "ans5- When dealing with a training set that has millions of instances and hundreds of features, it is generally recommended to use the dual form of the SVM problem. The dual form is more efficient in terms of memory and computational requirements for large-scale datasets. It allows for the use of kernel functions to handle high-dimensional feature spaces without explicitly computing the feature vectors. The primal form, on the other hand, can become computationally expensive and memory-intensive in such scenarios. Therefore, the dual form is preferred for training SVM models on large datasets with high-dimensional features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "que6- Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae674f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b3b649e",
   "metadata": {},
   "source": [
    "que7- To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43aab48",
   "metadata": {},
   "source": [
    "ans7- To solve the soft margin linear SVM classifier problem using a quadratic programming (QP) solver, the QP parameters should be set as follows:\n",
    "\n",
    "- **H:** The Hessian matrix (size: n x n) should be set to the identity matrix multiplied by a small regularization constant (C). This penalizes the misclassification errors and controls the trade-off between the margin and the training errors.\n",
    "\n",
    "- **f:** The linear coefficient vector (size: n x 1) should be set to all zeros since the objective function of the SVM is only concerned with minimizing the misclassification errors.\n",
    "\n",
    "- **A:** The matrix (size: m x n) should be set to the negated feature matrix (X) multiplied by the corresponding class labels (y) for the training instances. Each row represents an instance, and each column represents a feature.\n",
    "\n",
    "- **b:** The vector (size: m x 1) should be set to -1 for all training instances since the inequality constraint in the soft margin formulation of the SVM requires that the predicted values are less than or equal to -1 for negative class instances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
